{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Linear Regression Algorithm with Numpy\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is a method for modeling the relationship between one or more independent or features variables and one dependent or target variable.\n",
    "There are two ways of looking at a Regression problem:\n",
    "*\t**The statistician’s paradigm:** The goal is to understand the underlying causal effect of changes in an independent variable on changes in a dependent variable. For example, you may want to model the impact of an additional year of schooling on latter's wages. Or maybe the effect of a \\$ X discount on customers' propensity to adopt a product.\n",
    "\n",
    "*\t**The predictive paradigm:** Common in machine learning. Used for prediction and pattern recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many useful resources explaining the concept behind regression so, I won't be going into much detail here. I will only cover the points necessary to understand the math and implement it yourself without the help of any machine learning library. A typical regression equation looks like this: \n",
    "\n",
    "$$ \\Large y = \\underbrace{\\alpha + \\beta_1 \\times x_1 + \\beta_2 \\times x_2 + \\cdots + \\beta_n \\times x_n}_{\\text{$\\widehat{y}$}} + \\epsilon \\tag{1.0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here , the right hand side variables ($x_1,x_2\\cdots etc.$) are called the feature variables which along with their respective weights/coefficients ($\\alpha,\\beta_1,\\beta_2\\cdots etc.$) tries to predict the best fit model. When we know the value of these coefficients, we can make predictions about the data that we haven't seen before. In other words, we can infer the target value from feature values.\n",
    "$y$ is the actual outcome variable you are trying to measure. $\\widehat{y}$ is the predicted outcome.\n",
    "\n",
    "> There is always some scope of error between the actual target value and the predicted value. The goal of Linear Regression is to find the coefficients in order to minimize this error $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Vector Representation of Linear Regression equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the math let's see if we can visualize the above equation (1.0) in a matrix-vector representation. The above equation represents one single record of data. Let's rewrite the equation in a little different way. The intercept $\\alpha$ can be thought of as a coefficient of x where x=1.\n",
    "\n",
    "$$ \\begin{align}\\large \\widehat{y} = \\alpha\\times 1+ \\beta_1 \\times x_1 + \\beta_2 \\times x_2 + \\cdots + \\beta_n \\times x_n \\end{align} \\tag{1.1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "\\widehat{y}\n",
    "\\end{bmatrix}_{1 \\times 1}=\n",
    "  \\begin{bmatrix}\n",
    "    1 & x_1 & x_2 & x_3 & ... & x_n \n",
    "  \\end{bmatrix}_{1 \\times n+1}\n",
    "  %\n",
    "  \\begin{bmatrix}\n",
    "    \\alpha \\\\\n",
    "    \\beta_1 \\\\\n",
    "    \\beta_2 \\\\\n",
    "    \\beta_3 \\\\\n",
    "     \\vdots \\\\\n",
    "    \\beta_n \\\\\n",
    "  \\end{bmatrix}_{n+1 \\times 1} \\tag{1.2}\n",
    "\\end{equation}\n",
    "\n",
    "Now, if we do a dot product of eq(1.2) we will get back eq(1.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a typical regression problem, we will have multitude of data. If we have m records with n feature each , the matrix-vector representation of the equation will look like this.\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "\\widehat{y_1} \\\\\n",
    "\\widehat{y_2} \\\\\n",
    "\\widehat{y_3} \\\\\n",
    "\\vdots \\\\\n",
    "\\widehat{y_m} \n",
    "\\end{bmatrix}_{ m \\times 1}=\n",
    "  \\begin{bmatrix}\n",
    "    1 & x_{11} & x_{12} & x_{13} & ... & x_{1n} \\\\\n",
    "    1 & x_{21} & x_{22} & x_{23} & ... & x_{2n} \\\\\n",
    "    1 & x_{31} & x_{32} & x_{33} & ... & x_{3n} \\\\\n",
    "    \\vdots \\\\\n",
    "    1 & x_{m1} & x_{m2} & x_{m3} & ... & x_{mn} \\\\\n",
    "  \\end{bmatrix}_{ m \\times n+1}\n",
    "  %\n",
    "  \\begin{bmatrix}\n",
    "    \\alpha \\\\\n",
    "    \\beta_1 \\\\\n",
    "    \\beta_2 \\\\\n",
    "    \\beta_3 \\\\\n",
    "     \\vdots \\\\\n",
    "    \\beta_n \\\\\n",
    "  \\end{bmatrix}_{ n+1 \\times 1} \\tag{1.3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did I emphasize on such representation? Because rather than calculating each weight individually, you will have a vector representation of the weights and calculate them in one shot much more efficiently. These weights are called parameters in machine learning lingo. Notice that the weight vector doesn't change its dimension with more data, that means the number of parameters to calculate doesn't change with the number of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned previously, the goal of regression problem is to calculate the weights such that the error between the predicted y and the actual y is minimum.\n",
    "\n",
    "Even though Linear Regression has an analytical solution $\\theta=(X^TX)^{-1}X^TY$ where $\\theta$ is a vector representing all the coefficients we need for each feature, for large feature sets the computation can get very complex. Also, for huge dataset, the data may not fit in the memory.\n",
    "\n",
    "The alternate solution to calculate parameters is based on maximum likelihood estimation. Maximum likelihood estimation is a method that determines values for the parameters of a model such that they maximize the likelihood that the process described by the model produced the data that were actually observed. \n",
    "\n",
    "Gradient descent is one such maximum likelihood estimation algorithm.\n",
    "\n",
    "The main reason why gradient descent is used for linear regression is that it is computationally cheaper (faster) to find the solution using the gradient descent in some cases. It is beyond the scope of this post to fully explain how gradient descent works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to estimate parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps we need to go through to compute the parameters. Each step will be followed by the corresponding code snippets. The entire code can be found at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Define your model and gather your data\n",
    "This is a very important step where you identify your dependent and feature variables.\n",
    "> Before you run the gradient descent algorithm on your dataset, you normalize the data. Normalization is a technique often applied as part of data preparation in machine learning pipeline which typically means rescaling the values into a range of [0,1] to boost the accuracy while lowering the cost (error). When features are all on the same scale, they will contribute equally to determining the direction for greatest gradient descent.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this post, let's assume we have a dataset of 100 records and two features. We will not use any existing dataset, rather we will synthetically generate our own data and randomly split it in 80:20 ratio between train and test set. For the two features, we have to calculate two coefficients and an intercept. The equation will look like: $$ \\begin{align}\\large \\widehat{y} = \\alpha\\times 1+ \\beta_1 \\times x_1 + \\beta_2 \\times x_2 \\end{align} $$\n",
    "The labels are generated taking $\\alpha=2$ ,$\\beta_1=0.1$,$\\beta_2=0.4$ and some Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)\n",
    "#Stack another vector of 1 horizontally\n",
    "ones = np.ones(100).reshape(100,1)\n",
    "X= np.hstack((ones,X))\n",
    "y = 2 +  .1 * np.random.randn(100, 1) +.4 * np.random.randn(100, 1)\n",
    "\n",
    "#Normalize the data \n",
    "mean = np.mean(X)\n",
    "sigma = np.std(X)\n",
    "X = (X-mean) / sigma\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = X[train_idx], y[train_idx]\n",
    "x_val, y_val = X[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, we initially generated X with 2 columns (for two features) and 100 rows (number of records) Then, we concatenated 1 to all X values for the intercept. The data is also normalized with mean and std. deviation before splitting into train and test set. At this point *y is $100\\times 1$ vector and X* is $100\\times3$ matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Initialize the Parameters\n",
    "Once we have identified your feature variables, we have an idea how many parameters you need to train. Since we have two feature and one intercept there are three parameters to be trained. The dimension of weight vector is $3\\times1$. Initialize the parameters to random values to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37673897],\n",
       "       [0.7495783 ],\n",
       "       [0.39298945]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.rand(X.shape[1],1)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define the loss/cost function that you want to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, the loss is the difference between the predicted value and the actual value.\n",
    "$$\\large error = (y_i - \\hat{y_i})$$\n",
    "\n",
    "For the linear regression problem, the loss is given by the Mean Square Error (MSE), that is, the average of all squared differences between labels (y) and predictions ($\\widehat{y}$).We take the square of the errors so that the positive and negative error doesn’t cancel out.N is the number of records, in our case N=100.\n",
    "\n",
    "$$\\large MSE  = \\frac{1}{N}\\sum_{n=1}^{N}(y - \\widehat{y})^2$$\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y,y_hat):\n",
    "    loss = np.mean((y-y_hat)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.  Compute the Gradients\n",
    "A gradient is a partial derivative that tells you how much a given quantity changes when you slightly vary some other quantity. In our case, how much does our MSE loss change when we vary each one of our parameters. We calculate the partial derivative of loss with respect to each parameter.\n",
    "\n",
    "$$ MSE  = \\frac{1}{N}\\sum_{n=1}^{N}(y - \\widehat{y})^2$$\n",
    "\n",
    "$$\\large MSE  = \\frac{1}{N}\\sum_{n=1}^{N}(y - (\\alpha + \\beta_1 * x_1 + \\beta_2 * x_2))^2$$\n",
    "\n",
    "$$ \\large \\frac{\\partial MSE}{\\partial \\alpha} = \\frac{1}{N}\\sum_{n=1}^{N}(2).(y - (\\alpha + \\beta_1 * x_1 + \\beta_2 * x_2)).(-1) = -\\frac{2}{N}\\sum_{n=1}^{N}(y - \\widehat{y})) $$\n",
    "\n",
    "$$ \\large \\frac{\\partial MSE}{\\partial \\beta_1} = \\frac{1}{N}\\sum_{n=1}^{N}(2).(y - (\\alpha + \\beta_1 * x_1 + \\beta_2 * x_2)).(-x_1) = -\\frac{2}{N}\\sum_{n=1}^{N}x_1(y - \\widehat{y})) $$\n",
    "\n",
    "$$ \\large \\frac{\\partial MSE}{\\partial \\beta_2} = \\frac{1}{N}\\sum_{n=1}^{N}(2).(y - (\\alpha + \\beta_1 * x_1 + \\beta_2 * x_2)).(-x_2) = -\\frac{2}{N}\\sum_{n=1}^{N}x_2(y - \\widehat{y})) $$  \n",
    "\n",
    "Here, we calculated the partial derivate individually by hand for each parameter. But in reality, we will calculate in one shot through  NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivative(X,y,y_hat):\n",
    "    n_samples=len(y)\n",
    "    pd= np.dot(X.T,(y-y_hat))*(-2/n_samples)\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/LR2.png\" alt=\"Vector Representation of Partial Derivative\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above the how easily you can calculate the partial derivative through NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Update the Parameters\n",
    "In the final step, we use the gradients to update the parameters. Since we are trying to minimize our losses, we reverse the sign of the gradient for the update.\n",
    "There is still another parameter to consider: the ***learning rate***, denoted by the Greek letter eta (that looks like the letter n), which is the multiplicative factor that we need to apply to the gradient for the parameter update. The learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.\n",
    "\n",
    "$$ \\large \\alpha = \\alpha - \\eta \\frac{\\partial MSE}{\\partial \\alpha} $$\n",
    "\n",
    "$$ \\large \\beta_1 = \\beta_1 - \\eta \\frac{\\partial MSE}{\\partial \\beta_1} $$\n",
    "$$ \\large \\beta_2 = \\beta_2 - \\eta \\frac{\\partial MSE}{\\partial \\beta_2} $$\n",
    "\n",
    "> **Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our model with respect to the loss gradient. The lower the value, the slower we travel along the downward slope.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Algorithm\n",
    "Let's write down the entire algorithm \n",
    "* Initialize parameters with random values\n",
    "> * LOOP till the stopping criteria is met:\n",
    "    1. Compute the predicted y\n",
    "    2. Compute loss\n",
    "    3. Compute partial derivative w.r.t parameters\n",
    "    4. Update parameters\n",
    "    5. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fit(n_iter,lr):\n",
    "    for i in range(n_iter):\n",
    "        y_hat=np.dot(X,W)\n",
    "        loss = compute_cost(y,y_hat)\n",
    "        pd=partial_derivative(X,y,y_hat)\n",
    "        W = W - lr*pd\n",
    "        i=i+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping criterion and update rule for Gradient Descent\n",
    "\n",
    "There are a few options on deciding when to stop updating parameters.\n",
    "\n",
    "*\tDefine a threshold and stop when the loss function for the training data is smaller than a threshold.\n",
    "*\tDefine a threshold and stop when the loss function for the validation data is smaller than a threshold.\n",
    "*\tWhen the total absolute difference in parameters w is smaller than a threshold.\n",
    "*\tSometimes, we don't care if we have the optimal parameters. We just want to improve the parameters we originally had. In such case, it's reasonable to preset a number of iterations over the training data and stop after that regardless of whether the objective function actually converged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the Linear Regression on our dataset. We will use 2000 iterations as our stopping criteria and use learning rate as 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class definition of Linear Regression\n",
    "class LinearRegression:\n",
    "    def __init__(self,X,y,lr=0.01,n_iter=2000):\n",
    "        self.n_samples = len(y)\n",
    "        self.W= np.random.rand(X.shape[1],1)\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.lr=lr\n",
    "        self.iter=n_iter\n",
    "        self.history = np.zeros((n_iter,1))\n",
    "   \n",
    "    #Compute cost\n",
    "    def compute_cost(self):\n",
    "        loss = np.mean((self.y-self.y_hat)**2)\n",
    "        return loss\n",
    "    \n",
    "    #Update the coefficients\n",
    "    def update_param(self):\n",
    "        self.W = self.W - self.lr*np.dot(self.X.T,(self.y-self.y_hat))*(-2/self.n_samples)\n",
    "        \n",
    "    def fit(self):\n",
    "        for i in range(self.iter):\n",
    "            self.y_hat=np.dot(self.X,self.W)\n",
    "            loss = self.compute_cost()\n",
    "            self.history[i]=loss\n",
    "            self.update_param()\n",
    "            i=i+1\n",
    "        print(\"Updated params for numpy regression: \",self.W.reshape(3,))\n",
    "     \n",
    "    def predict(X_predict):        \n",
    "        y_predict = np.dot(X_predict,self.W)\n",
    "        return y_predict\n",
    "    \n",
    "    def getAllLoss(self):\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated params for numpy regression:  [1.99931273 0.00640892 0.01194574]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(x_train,y_train)\n",
    "lr.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Lets plot the graph to see how the losses have been behaving with each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm8HGWd7/HPNzlJ2EJYEllCIILIolcQwuIIDo6iwCBRRAEZFkGZUVERdAQdlYteHfXKdUFFFESUVURFB1RUELiXLUQIS1gCwhAJJKwJBAIhv/vH83RTp9PbSU6d7lDf9+tVr9NdVV316+o+9e3nqepqRQRmZmYAo3pdgJmZ9Q+HgpmZ1TkUzMyszqFgZmZ1DgUzM6tzKJiZWZ1DwawEko6QdM0wLWt1Sb+R9JSknw/HMl9OJF0m6fBe1/Fy4VDoY5LeJ2mGpKclzctv/t16XdeqSNKekq6QtEjSY5JulvRpSav1urYuHABsAKwfEe9pnCjpJEk/G/myQFJIeia/R5+W9GTJ61vuuUbE3hHxkzLXWyUOhT4l6Tjgm8CXSTuETYHvAdN7WVeRpIFe19ANSe8BLgLOBTaLiPWBA4FNgCktHtNPz20z4O6IWNrrQlrYLiLWysM6vS7GVlJEeOizAZgAPA28p80840ih8VAevgmMy9P2AOYCxwPzgXnA+/O0XYGHgdGFZb0LmJVvjwJOAO4FHgMuBNbL06YCARwF/DdwVR5/GPBAnv9zwP3AW4ewvMPz8h4FPluoazTwmfzYRcBNwJQ8bWvgcuBx4C7gvS22k4AHgeM7bPOTSMHxM2Ah8AFgZ+Ba4Mm8DU8FxhYeE8DHgPty7V8HRuVpRwDXAP8beAL4G7B3m/VvA1yZ13U7sF8e/z+B54EX8nviqBa1/2woy83T9gHuyNv278An8/iJwG/zYx4Hrq49rybLD+BVTcYfAVzTal7gLOC7wH/l9V8PbFGY9zWF1/eR/D7Yq2Fb3JLnvRL4QOH99h+k9+N84GxgQjfvNw952/e6AA9NXpT05l8KDLSZ52TgOuAVwCTg/wFfzNP2yI8/GRiT//kXA+vm6fcCexaW9XPghHz72LzcTUjB8wPgvDyt9k91NrAmsDqwbf4H3Q0YS9oJvsBLodDN8n6Yl7UdsATYJk//FHArsBVp574dsH5e94PA+4EBYIf8D/6aJttp67yOqR22+Um57nfmHcvqwI6kEB3Itc4Gji08JoArgPVILbm7CzunI/LyPkgKtw+RwltN1j0GmEPa8Y0F/om0o9yqUFvTnX676V0sdx6we769LrBDvv0V4LT8+DHA7s3qLmyDFQ2Fx0nBOwCcA5yfp43PtR0PrJbv79LquTI4FI7Mz3lzYC3gYuCn3bzfPOTt2esCPDR5UeAQ4OEO89wL7FO4/3bg/nx7D+BZCqFC+tS0a779JeDMfHs88AypWwXSju8thcdtlHdutR1jAJsXpn+evJPP99cgfZp76xCWt0lh+g3AQfn2XcD0Js/9QODqhnE/AL7QZN7d8jpWK4w7n/QpeDFwaB53Ernl02abHwv8snA/gL0K9z8M/CnfPgKY07BdAtiwyXJ3J7XeRhXGnQecVKhtRUKh03L/G/hXYO2Gx50M/JomO/sm6whSy+rJPHy78Pw7hcKPCtP2Ae7Mtw8G/trtc2VwKPwJ+HBh2lbdvt88pMHHFPrTY8DEDv3aG5OayDUP5HH1ZcTgPujFpE9OkPrW95c0DtgfmBkRtWVtBvxS0pP5oOFs4EXScY2aBxvqqN+PiMW5/ppulvdwizqnkMKv0WbALrVl5uUeAmzYZN5aLRsVajwoUt/3TNKn+GbPC0mvlvRbSQ9LWkg6vjOxYfnFxzS+BvXnlbcLhedWtDHwYEQsa1jW5CbzDkWn5b6btDN+QNJfJL0hj/866dP2HyTdJ+mEDuvZISLWycPHhlDfUF/3bjT7vxigu/eb4QPN/epa4DlSV0YrD5F2jjWb5nEdRcQdpH+WvYH3kUKi5kFS3/c6hWG1iPh7cRGF2/NIXUNAOn2S1MUzlOW18iCwRYvxf2lY5loR8aEm895J6i/fv4v1RcP97+fHbxkRa5O6YdQwT/FAddevQYOHgCmSiv+Pm5LqXhltlxsRN0bEdFIX5K9Ix3uIiEURcXxEbA68AzhO0luGuO5nSK0jACQ1C+xWWr3usPxr1KjZ/8VS0nEJ64JDoQ9FxFOkbpnvSnqnpDUkjZG0t6Sv5dnOA/5D0iRJE/P8Qzkt8VzSQdI3kY4p1JwG/C9JmwHk5bc74+ki4B2S/kHSWNKB0eKOc6jLK/oR8EVJWyp5naT1SQdBXy3p0LxdxkjaSdI2jQuI1EdwPPAFSR+UtG5e1pYM/vTYzHhS18jTkrYmHRdo9Km8zCnAx4ELunxuRdeTdqL/np/LHqSd8flDWMYoSasVhnHtlitprKRDJE2IiBfy83wRQNK+kl4lSYXxLw7xOd0CvEbS9vm035OG8NjfAhtKOlbSOEnjJe2Spz0CTG0IuqLzgE9IeqWktUituwuif8/c6jsOhT4VEacAx5HOpFhA+vR0DOkTHaTjAjOAWaSDsTPzuG6dRzr28OeIeLQw/lvAJaSug0Wkg8S7LP/wep23Ax8l7cDmkQ5kzicdwBvy8hqcQvr0+gfSzukMYPWIWAS8DTiI9MnwYeCrpAPZzWq8AHgv8C+k7fhoXu7pDA7ERp8ktaQWkQ5ONtvh/5p0VtTNpDNpzujyuRXrex7Yj9Rye5R06vFhEXHnEBZzMOk4Um24t4vlHgrcn7vG/o20fQC2BP5IOoHgWuB7EXHlEJ/T3aRjE38E7iGdidXtYxcBe5IC7OH8+DfnybXX6zFJM5s8/Ezgp8BVpDO+niO9P61LygdbzIZF/nT2JKnL5W+9rqdMkoL0POf0uhaz4eKWgq00Se/IXVxrkk5JvZX0XQUzW8U4FGw4TOelL9FtSTrFz01Qs1WQu4/MzKzOLQUzM6vrp4t+dWXixIkxderUXpdhZrZKuemmmx6NiEmd5lvlQmHq1KnMmDGj12WYma1SJD3QeS53H5mZWYFDwczM6hwKZmZW51AwM7M6h4KZmdU5FMzMrM6hYGZmddUJhdtug899DubP73UlZmZ9qzqhcOed8KUvORTMzNqoTiiMzj/Fu9Q/wGRm1kr1QuHFof6qoJlZdVQnFAbyZZ4cCmZmLVUnFNx9ZGbWUfVCwS0FM7OWqhMK7j4yM+uotFCQNEXSFZJmS7pd0sebzLOHpKck3ZyHz5dVj7uPzMw6K/NHdpYCx0fETEnjgZskXR4RdzTMd3VE7FtiHYm7j8zMOiqtpRAR8yJiZr69CJgNTC5rfR25+8jMrKMROaYgaSrweuD6JpPfIOkWSZdJek2Lxx8taYakGQsWLFixItx9ZGbWUemhIGkt4BfAsRGxsGHyTGCziNgO+A7wq2bLiIjTI2JaREybNKnj70435+4jM7OOSg0FSWNIgXBORFzcOD0iFkbE0/n2pcAYSRNLKcbdR2ZmHZV59pGAM4DZEXFKi3k2zPMhaedcz2OlFOTuIzOzjso8++iNwKHArZJuzuM+A2wKEBGnAQcAH5K0FHgWOCgiopRq3H1kZtZRaaEQEdcA6jDPqcCpZdUwiLuPzMw6qs43mt19ZGbWUfVCwS0FM7OWqhMK7j4yM+uoOqHg7iMzs46qFwpuKZiZtVSdUHD3kZlZR9UJBXcfmZl1VL1QcEvBzKyl6oSCu4/MzDqqTii4+8jMrKPqhIKUBrcUzMxaqk4oQOpCckvBzKylaoXC6NFuKZiZteFQMDOzumqFgruPzMzaqlYouKVgZtZWtUJhYMChYGbWRrVCYfRodx+ZmbVRvVBwS8HMrKVqhYK7j8zM2qpWKLj7yMysreqFglsKZmYtVSsU3H1kZtZWtULB3UdmZm1VLxTcUjAza6laoeDuIzOztqoVCu4+MjNrq3qh4JaCmVlL1QoFdx+ZmbVVrVBw95GZWVvVCwW3FMzMWiotFCRNkXSFpNmSbpf08SbzSNK3Jc2RNEvSDmXVA7j7yMysg4ESl70UOD4iZkoaD9wk6fKIuKMwz97AlnnYBfh+/lsOdx+ZmbVVWkshIuZFxMx8exEwG5jcMNt04OxIrgPWkbRRWTW5+8jMrL0ROaYgaSrweuD6hkmTgQcL9+eyfHAg6WhJMyTNWLBgwYoX4u4jM7O2Sg8FSWsBvwCOjYiFjZObPCSWGxFxekRMi4hpkyZNWvFi3H1kZtZWqaEgaQwpEM6JiIubzDIXmFK4vwnwUGkFufvIzKytMs8+EnAGMDsiTmkx2yXAYfkspF2BpyJiXlk1ufvIzKy9Ms8+eiNwKHCrpJvzuM8AmwJExGnApcA+wBxgMfD+Eutx95GZWQelhUJEXEPzYwbFeQL4SFk1LMfdR2ZmbVXrG80DA24pmJm1Ua1QcEvBzKytaoWCWwpmZm05FMzMrK5aoTBmDLzwQq+rMDPrWw4FMzOrq14oLFuWBjMzW071QgHcWjAza6FaoTCQv6vng81mZk1VKxTcUjAza8uhYGZmdQ4FMzOrcyiYmVmdQ8HMzOqqFQo++8jMrK1qhYJbCmZmbTkUzMyszqFgZmZ1DgUzM6tzKJiZWV21QsFnH5mZtVWtUHBLwcysLYeCmZnVORTMzKzOoWBmZnXVDAUfaDYza6paoVA7+8gtBTOzpqoVCu4+MjNry6FgZmZ1DgUzM6srLRQknSlpvqTbWkzfQ9JTkm7Ow+fLqqXOoWBm1tZAics+CzgVOLvNPFdHxL4l1jCYL3NhZtZWaS2FiLgKeLys5a8QtxTMzNrq9TGFN0i6RdJlkl7TaiZJR0uaIWnGggULVnxto0alwaFgZtZUL0NhJrBZRGwHfAf4VasZI+L0iJgWEdMmTZq0cmsdM8ahYGbWQlehIOmn3YwbiohYGBFP59uXAmMkTVyZZXbFoWBm1lK3LYVBXTuSRgM7rsyKJW0oSfn2zrmWx1ZmmV1xKJiZtdT27CNJJwKfAVaXtLA2GngeOL3DY88D9gAmSpoLfAEYAxARpwEHAB+StBR4FjgoImLFn0qXBgZ89pGZWQttQyEivgJ8RdJXIuLEoSw4Ig7uMP1U0imrI8stBTOzlrrtPvqtpDUBJP2LpFMkbVZiXeVxKJiZtdRtKHwfWCxpO+DfgQdo/6W0/uVQMDNrqdtQWJr7+6cD34qIbwHjyyurRA4FM7OWur3MxaJ80PlQYPd89tGY8soqkUPBzKylblsKBwJLgCMj4mFgMvD10qoq09ix8Pzzva7CzKwvdRUKOQjOASZI2hd4LiJWzWMK48bBkiW9rsLMrC91+43m9wI3AO8B3gtcL+mAMgsrjUPBzKylbo8pfBbYKSLmA0iaBPwRuKiswkozbhwsXNh5PjOzCur2mMKoWiBkjw3hsf1l7Fi3FMzMWui2pfA7Sb8Hzsv3DwQuLaekkrn7yMyspU7XPnoVsEFEfErS/sBupGsfXUs68LzqcSiYmbXUqQvom8AigIi4OCKOi4hPkFoJ3yy7uFKMG+dTUs3MWugUClMjYlbjyIiYAUwtpaKyuaVgZtZSp1BYrc201YezkBHjUDAza6lTKNwo6YONIyUdBdxUTkklcyiYmbXU6eyjY4FfSjqEl0JgGjAWeFeZhZVm3Lj0IzvLlsGoVfOsWjOzsnT6kZ1HgH+Q9GbgtXn0f0XEn0uvrCzjxqW/S5bA6qtmD5iZWVm6+p5CRFwBXFFyLSPDoWBm1lL1+k+KoWBmZoM4FMzMrM6hYGZmdQ4FMzOrcyiYmVld9UJh7Nj016FgZrac6oWCWwpmZi05FMzMrK66oeDLZ5uZLae6oeCWgpnZchwKZmZW51AwM7M6h4KZmdWVFgqSzpQ0X9JtLaZL0rclzZE0S9IOZdUyiEPBzKylMlsKZwF7tZm+N7BlHo4Gvl9iLS+pXS77uedGZHVmZquS0kIhIq4CHm8zy3Tg7EiuA9aRtFFZ9dSNHZt+cW3x4tJXZWa2qunlMYXJwIOF+3PzuOVIOlrSDEkzFixYsHJrlWCNNRwKZmZN9DIU1GRcNJsxIk6PiGkRMW3SpEkrv2aHgplZU70MhbnAlML9TYCHRmTNDgUzs6Z6GQqXAIfls5B2BZ6KiHkjsmaHgplZUwNlLVjSecAewERJc4EvAGMAIuI04FJgH2AOsBh4f1m1LGf11R0KZmZNlBYKEXFwh+kBfKSs9bflloKZWVPV+0YzOBTMzFqobig8+2yvqzAz6zvVDQW3FMzMluNQMDOzOoeCmZnVORTMzKyuuqHwwgtpMDOzuuqGAvgMJDOzBtUOBXchmZkN4lAwM7O6aoeCu4/MzAapdig880xv6zAz6zPVDIXx49PfRYt6W4eZWZ+pZiisvXb6+9RTva3DzKzPVDMUJkxIfxcu7G0dZmZ9ppqhUGspOBTMzAapZijUjik4FMzMBqlmKIwZk36S06FgZjZINUMBUheSDzSbmQ1S7VBwS8HMbJDqhsKECQ4FM7MG1Q0FtxTMzJbjUDAzs7pqh4IPNJuZDVLtUHBLwcxskOqGwoQJqaWwbFmvKzEz6xvVDYWJE1MgPPlkrysxM+sb1Q2FSZPS30cf7W0dZmZ9pLqhMHFi+utQMDOrcyg4FMzM6koNBUl7SbpL0hxJJzSZfoSkBZJuzsMHyqxnkFooLFgwYqs0M+t3A2UtWNJo4LvAnsBc4EZJl0TEHQ2zXhARx5RVR0tuKZiZLafMlsLOwJyIuC8ingfOB6aXuL6hWWMNWG01h4KZWUGZoTAZeLBwf24e1+jdkmZJukjSlBLrGUxKrQWHgplZXZmhoCbjouH+b4CpEfE64I/AT5ouSDpa0gxJMxYM5zGASZN8TMHMrKDMUJgLFD/5bwI8VJwhIh6LiCX57g+BHZstKCJOj4hpETFtUu37BcNhgw1g3rzhW56Z2SquzFC4EdhS0isljQUOAi4pziBpo8Ld/YDZJdazvClTYO7cEV2lmVk/K+3so4hYKukY4PfAaODMiLhd0snAjIi4BPiYpP2ApcDjwBFl1dPUlCkwfz4sWQLjxo3oqs3M+lFpoQAQEZcClzaM+3zh9onAiWXW0NYmm6S/c+fCFlv0rAwzs35R3W80Q2opgLuQzMyyaodCraXw4IPt5zMzq4hqh0KtpeBQMDMDqh4Ka64JG24I99zT60rMzPpCtUMBYOut4c47e12FmVlfcCjUQiEav2xtZlY9DoWtt4YnnvDlLszMcCikUAC4/fbe1mFm1gccCjvskP7eeGNv6zAz6wMOhUmT0reZr7++15WYmfWcQwFg113h2mt9sNnMKs+hALDbbukS2j411cwqzqEA8M//nP7+5je9rcPMrMccCpAud7H99nDxxb2uxMyspxwKNYcdlg4233JLrysxM+sZh0LN4YfD6qvD177W60rMzHrGoVCz3nrwiU/AueemM5HMzCrIoVB0wgnp+ML73gePPtrraszMRpxDoWj8eLjoonR66pveBH/7W68rMjMbUQ6FRjvvDL/7Hfz97/C616VjDIsW9boqM7MR4VBoZo89YNYs2H13+PSnU5fSkUfCpZc6IMzsZW2g1wX0rc02SyFwww1w6qnwi1/Aj38Mo0al7zTstBNsuy1ssw1stRVsvDEMeHOa2arNe7FOdt4Zzj4bliyBq66Cq6+Ga66BCy9Mv8NQM2oUbLABTJ6chvXXh3XWgXXXfenv2mun015XWy0Ntdu1v2PHwujRLw2jRoHUu+duZpXjUOjWuHGw555pgHTxvPnzYfZsuPvudAxi7tz09957YcaMFBqLF6/ceoshURsGBgaHRi042v1d2XkcTi8Pfh1XbR/4ABx3XKmrcCisKCm1DDbYIB2DaOX55+HJJ9OwcCE8+yw891waGm8//zy8+OLgYenS1uOWLXvpyq7t/q7sPL567MuDX8dV3wYblL4Kh0LZxo6FV7wiDWZmfc5nH5mZWZ1DwczM6hwKZmZW51AwM7M6h4KZmdWVGgqS9pJ0l6Q5kk5oMn2cpAvy9OslTS2zHjMza6+0UJA0GvgusDewLXCwpG0bZjsKeCIiXgX8H+CrZdVjZmadldlS2BmYExH3RcTzwPnA9IZ5pgM/ybcvAt4i+SuXZma9UuaX1yYDDxbuzwV2aTVPRCyV9BSwPjDoF24kHQ0cne8+LemuFaxpYuOy+0S/1gX9W5vrGhrXNTQvx7o262amMkOh2Sf+xu/ZdzMPEXE6cPpKFyTNiIhpK7uc4davdUH/1ua6hsZ1DU2V6yqz+2guMKVwfxPgoVbzSBoAJgCPl1iTmZm1UWYo3AhsKemVksYCBwGXNMxzCXB4vn0A8OcIX7XLzKxXSus+yscIjgF+D4wGzoyI2yWdDMyIiEuAM4CfSppDaiEcVFY92Up3QZWkX+uC/q3NdQ2N6xqaytYlfzA3M7Maf6PZzMzqHApmZlZXmVDodMmNktc9RdIVkmZLul3Sx/P4kyT9XdLNedin8JgTc613SXp7ibXdL+nWvP4Zedx6ki6XdE/+u24eL0nfznXNkrRDSTVtVdgmN0taKOnYXmwvSWdKmi/ptsK4IW8fSYfn+e+RdHizdQ1DXV+XdGde9y8lrZPHT5X0bGG7nVZ4zI759Z+Ta1+pL4+2qGvIr9tw/7+2qOuCQk33S7o5jx/J7dVq39C791hEvOwH0oHue4HNgbHALcC2I7j+jYAd8u3xwN2kS3+cBHyyyfzb5hrHAa/MtY8uqbb7gYkN474GnJBvnwB8Nd/eB7iM9P2SXYHrR+i1e5j0xZsR317Am4AdgNtWdPsA6wH35b/r5tvrllDX24CBfPurhbqmFudrWM4NwBtyzZcBe5dQ15BetzL+X5vV1TD9G8Dne7C9Wu0bevYeq0pLoZtLbpQmIuZFxMx8exEwm/Rt7lamA+dHxJKI+Bswh/QcRkrx8iM/Ad5ZGH92JNcB60jaqORa3gLcGxEPtJmntO0VEVex/Hdnhrp93g5cHhGPR8QTwOXAXsNdV0T8ISKW5rvXkb4b1FKube2IuDbSnuXswnMZtrraaPW6Dfv/a7u68qf99wLntVtGSdur1b6hZ++xqoRCs0tutNspl0bpSrCvB67Po47JzcAza01ERrbeAP4g6Saly4kAbBAR8yC9aYHaD0z3YjsexOB/1l5vLxj69unFdjuS9Imy5pWS/irpL5J2z+Mm51pGoq6hvG4jvb12Bx6JiHsK40Z8ezXsG3r2HqtKKHR1OY3Si5DWAn4BHBsRC4HvA1sA2wPzSE1YGNl63xgRO5CuZvsRSW9qM++IbkelLz3uB/w8j+qH7dVOqzpGert9FlgKnJNHzQM2jYjXA8cB50paewTrGurrNtKv58EM/uAx4turyb6h5awtahi22qoSCt1ccqNUksaQXvRzIuJigIh4JCJejIhlwA95qctjxOqNiIfy3/nAL3MNj9S6hfLf+SNdV7Y3MDMiHsk19nx7ZUPdPiNWXz7AuC9wSO7iIHfPPJZv30Tqr391rqvYxVRKXSvwuo3k9hoA9gcuKNQ7otur2b6BHr7HqhIK3VxyozS5z/IMYHZEnFIYX+yPfxdQOzPiEuAgpR8heiWwJekA13DXtaak8bXbpAOVtzH48iOHA78u1HVYPgNiV+CpWhO3JIM+wfV6exUMdfv8HnibpHVz18nb8rhhJWkv4NPAfhGxuDB+ktLvmyBpc9L2uS/XtkjSrvk9eljhuQxnXUN93Uby//WtwJ0RUe8WGsnt1WrfQC/fYytz5HxVGkhH7e8mpf5nR3jdu5GacrOAm/OwD/BT4NY8/hJgo8JjPptrvYuVPMOhTV2bk87suAW4vbZdSJcv/xNwT/67Xh4v0g8n3ZvrnlbiNlsDeAyYUBg34tuLFErzgBdIn8aOWpHtQ+rjn5OH95dU1xxSv3LtPXZanvfd+fW9BZgJvKOwnGmknfS9wKnkqxwMc11Dft2G+/+1WV15/FnAvzXMO5Lbq9W+oWfvMV/mwszM6qrSfWRmZl1wKJiZWZ1DwczM6hwKZmZW51AwM7M6h4L1nKSQ9I3C/U9KOmmYln2WpAOGY1kd1vMepStdXtEwfqrylTklba/CFUKHYZ3rSPpw4f7Gki4aruVbNTkUrB8sAfaXNLHXhRTVvsDUpaOAD0fEm9vMsz3pHPSh1NDuJ3PXAeqhEBEPRUTpAWgvbw4F6wdLSb89+4nGCY2f9CU9nf/ukS9WdqGkuyX9p6RDJN2gdL37LQqLeaukq/N8++bHj1b6/YEb84Xa/rWw3CsknUv6clBjPQfn5d8m6at53OdJX0I6TdLXmz3B/M3ck4EDla7Rf2D+RvmZuYa/Spqe5z1C0s8l/YZ0scK1JP1J0sy87toVQ/8T2CIv7+sNrZLVJP04z/9XSW8uLPtiSb9Tuu7+1wrb46z8vG6VtNxrYdXQ7lOI2Uj6LjCrtpPq0nbANqRLIt8H/Cgidlb6oZKPAsfm+aYC/0i6KNsVkl5FukTBUxGxk6RxwP+V9Ic8/87AayNdzrlO0sak3ynYEXiCtMN+Z0ScLOmfSL8ZMKNZoRHxfA6PaRFxTF7el4E/R8SRSj+Ic4OkP+aHvAF4XUQ8nlsL74qIhbk1dZ2kS0jX2X9tRGyflze1sMqP5PX+D0lb51pfnadtT7oa5xLgLknfIV2Fc3JEvDYva532m95ertxSsL4Q6cqQZwMfG8LDbox0PfolpK/913bqt5KCoObCiFgW6dLI9wFbk64Nc5jSr21dT7qswJZ5/hsaAyHbCbgyIhZE+t2Cc0g/3rKi3gackGu4ElgN2DRPuzwiatf/F/BlSbOAP5IuibxBh2XvRrq8BBFxJ/AA6aJuAH+KiKci4jngDtIPGN0HbC7pO0rXUGp3pU57GXNLwfrJN0nXmvlxYdxS8oeXfPGwsYVpSwq3lxXuL2Pwe7vxWi61Sw1/NCIGXTRM0h7AMy3qW6mfXmyxvHdHxF0NNezSUMMhwCRgx4h4QdL9pADptOxWitvtRdKvtT0haTvSj7V8hPSjM0d29SzsZcUtBesb+ZPxhaSDtjX3k7prIP3q1JgVWPR7JI3Kxxk2J1187ffAh5QuW4ykVytdKbad64F/lDQxH4Q+GPjLEOpBbTyaAAAA40lEQVRYRPrJxZrfAx/NYYek17d43ARgfg6EN5M+2TdbXtFVpDAhdxttSnreTeVuqVER8Qvgc6SfrrQKcihYv/kGUDwL6YekHfENQOMn6G7dRdp5X0a6IuZzwI9IXScz88HZH9Ch5RzpEsUnAleQr6AZEUO5dPIVwLa1A83AF0khNyvX8MUWjzsHmCZpBmlHf2eu5zHSsZDbmhzg/h4wWtKtpN8KOCJ3s7UyGbgyd2WdlZ+nVZCvkmpmZnVuKZiZWZ1DwczM6hwKZmZW51AwM7M6h4KZmdU5FMzMrM6hYGZmdf8fSCFFbhVlzUEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history= lr.getAllLoss()\n",
    "plt.plot(range(len(history)), history, 'r')\n",
    "\n",
    "plt.title(\"Convergence Graph of Loss Function\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the cost decreased drastically with more iterations. The gradient descent function returned the optimal parameter values, consequently, we can now use them to predict new target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Our Implementation with Sklearn's Linear Regression\n",
    "\n",
    "We will now run our training data on sklearn model and compare the results. One thing worth mentioning is since we had concatenated 1 in our model, sklearn will treat it as a coefficient and hence intercept is set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated params for sklearn regression:  [[1.99931284 0.00640898 0.01194582]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression(fit_intercept=False)\n",
    "linr.fit(x_train, y_train)\n",
    "print(\"updated params for sklearn regression: \",linr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the parameters calculated by our program match with sklearn's model up to 6 digits of decimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}